{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the ones that we used for the base model\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Your name and file comment here: Jessica Bae\n",
    "\"\"\"\n",
    "\n",
    "def generate_tuples_from_file(training_file_path):\n",
    "    #use with statement to open the file\n",
    "    with open(training_file_path, 'r') as file: #read the file\n",
    "        lines = file.readlines() #read each line in the file\n",
    "        #iterate over and create a list of tuples\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        info = l.split('\\t') #split each line by the tab\n",
    "        data.append((info[0], info[1], int(info[2]))) #info[0] is id, info[1] is sentence, info[2] is the label\n",
    "    file.close()\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def precision(gold_labels, classified_labels):\n",
    "    #true pos/ true pos + false pos\n",
    "    truepos = 0\n",
    "    falsepos = 0\n",
    "    for i in range (len(gold_labels)):\n",
    "        if gold_labels[i] == 1 and classified_labels[i] == 1: #truepos is when system and gold positive\n",
    "            truepos += 1\n",
    "        if gold_labels[i] == 0 and classified_labels[i] == 1: #falsepos is when system is positive but gold negative\n",
    "            falsepos += 1\n",
    "    return (truepos/ (truepos + falsepos))\n",
    "\n",
    "def recall(gold_labels, classified_labels):\n",
    "    #true pos/ true pos + false neg\n",
    "    truepos = 0\n",
    "    falseneg = 0\n",
    "    for i in range (len(gold_labels)):\n",
    "        if gold_labels[i] == 1 and classified_labels[i] == 1:  #truepos is when system and gold positive\n",
    "            truepos += 1\n",
    "        if gold_labels[i] == 1 and classified_labels[i] == 0: #falseneg is when gold is positive but system negative\n",
    "            falseneg += 1\n",
    "    return(truepos / (truepos + falseneg))\n",
    "\n",
    "def f1(gold_labels, classified_labels):\n",
    "    #2 * precision * recall/ precision + recall\n",
    "    pre = precision(gold_labels, classified_labels)\n",
    "    re = recall(gold_labels, classified_labels)\n",
    "    return((2 * pre * re)/ (pre + re))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Implement any other non-required functions here\n",
    "\"\"\"\n",
    "\n",
    "def generate_test(training_file_path):\n",
    "    #use with statement to open the file\n",
    "    with open(training_file_path, 'r', encoding=\"utf8\") as file: #read the file\n",
    "        lines = file.readlines() #read each line in the file\n",
    "        #iterate over and create a list of tuples\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        info = l.split('\\t') #split each line by the tab\n",
    "        data.append((info[0], info[1])) #info[0] is id, info[1] is sentence NO INFO[2] becuase we classify these\n",
    "    file.close()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def punctuation(string): \n",
    "  \n",
    "    # punctuation marks \n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "  \n",
    "    # traverse the given string and if any punctuation \n",
    "    # marks occur replace it with null \n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "  \n",
    "    # Print string without punctuation \n",
    "    return string\n",
    "\n",
    "\"\"\"\n",
    "implement your SentimentAnalysis class here\n",
    "\"\"\"\n",
    "class SentimentAnalysis:\n",
    "    \n",
    "    def __init__(self):\n",
    "    # do whatever you need to do to set up your class here\n",
    "    \n",
    "        self.pos_count = {} #dictionary for pos_count -word count respect to positive comments (freq)\n",
    "        self.neg_count = {} #dictionary for neg_count -word count respect to negative comments (freq)\n",
    "        \n",
    "        self.vocab = 0 #number of words in our vocabulary\n",
    "        self.pos = 0 #class count for probability calculations \n",
    "        self.neg = 0 #class count for probability calculations\n",
    "        \n",
    "        self.pos_prob = {} #probability list for each positive word \n",
    "        self.neg_prob = {} #probability list for each negative word\n",
    "        \n",
    "        self.pos_c_prob = 0 #positive class probability\n",
    "        self.neg_c_prob = 0 #negative class probability\n",
    "        \n",
    "        self.score_class = {} #score return of dictionary P(c)*P(data|c)  for each class\n",
    "        \n",
    "\n",
    "        self.gold_labels = [0,0,0,1,0,0,1,1,1,1,1,0,1,0,0,0,1,0,1,0,0,1,1,0,1,1,0,1,1,1,0,0,0,0,1,0,1,1,1,1,0,0,1,0,1,1,1,1,1,0]\n",
    "        self.classified_labels = []\n",
    "        #To calculate gold or actual values, I went ahead and went through the test data and labeled pos/neg\n",
    " \n",
    "        print(\"initialized\\n\")\n",
    "\n",
    "    def train(self, examples): #examples is the generated tuple we did above\n",
    "        for e in examples: #loop through all our examples\n",
    "            feat = self.featurize(e) #get our features using featurize= create a bag of word features (w1,1) -> (x,y)\n",
    "            #the example e can be positive or negative \n",
    "            if e[2] == 1: #if our label is 1--positive\n",
    "                self.pos += 1 #we want to add it to our positive count (class count) how many + classes we saw\n",
    "                for f in feat: #feat is a list of tuple, we need to look at each f in feat\n",
    "                    if f[0] not in self.pos_count: #if the token in that f is not present in dictionary\n",
    "                        #the first time we are encountering the word\n",
    "                        self.vocab += 1 #keep track of our vocab\n",
    "                        self.pos_count[f[0]] = 0 #put a zero for pos_count \n",
    "                        self.pos_count[f[0]] += 1 #increment count by one\n",
    "                    else: #if token is already present in dictionary we want to increment the count of token\n",
    "                        self.pos_count[f[0]] += 1\n",
    "                       \n",
    "            if e[2] == 0: #if our label is 0--negative\n",
    "                self.neg += 1 #we want to add it to our negative count (class count)\n",
    "                for f in feat:#repeat above\n",
    "                    if f[0] not in self.neg_count:\n",
    "                        self.neg_count[f[0]] = 0 #put a zero for neg_count \n",
    "                        self.neg_count[f[0]] += 1 #increment count by one\n",
    "                    else: \n",
    "                        self.neg_count[f[0]] += 1\n",
    "       \n",
    "        #we need to make sure that both dictionary has both words - dictionary needs same set of words\n",
    "        #If not in both, discard the word or handle the situation by adding to the other dictionary with a 0 count\n",
    "        \n",
    "        for k,v in self.neg_count.items(): #iterate over negative dictionary\n",
    "            if k not in self.pos_count: #if k is not present in positive dictionary\n",
    "                self.vocab += 1 #sum up the rest of vocab count with neg words as well\n",
    "                self.pos_count[k] = 0 #add k of neg vocab with a 0 count - important for laplace smoothing\n",
    "        for k,v in self.pos_count.items(): #do the same thing but we dont need to count vocab since we have all - and +\n",
    "            if k not in self.neg_count:\n",
    "                self.neg_count[k] = 0 #add k of pos vocab with a 0 count to neg_count \n",
    "        \n",
    "        #calculate probabilities\n",
    "        #the word count of w1 + 1/all the words in positive context + |v|\n",
    "        for k,v in self.pos_count.items():\n",
    "            self.pos_prob[k] = (self.pos_count[k] + 1)/(self.pos + self.vocab)\n",
    "            \n",
    "        for k,v in self.neg_count.items(): #same with words in negative context + |v|\n",
    "            self.neg_prob[k] = (self.neg_count[k] + 1)/(self.neg + self.vocab)\n",
    "        \n",
    "        #calculate class probabilities\n",
    "        self.pos_c_prob = self.pos/(self.pos + self.neg) #take all pos class counts/ total counts\n",
    "        self.neg_c_prob = self.neg/(self.pos + self.neg) #take all neg class counts/ total counts\n",
    "\n",
    "    def score(self, data):\n",
    "        \n",
    "        data_p = (0, data, 1) #create a tuple variable with id, sentence, and positive label\n",
    "        data_n = (0, data, 0) #create a tuple variable with id, sentence, and negative label\n",
    "        \n",
    "        feat_p = self.featurize(data_p) #featurize each positive and negative sentence data\n",
    "        feat_n = self.featurize(data_n)\n",
    "        \n",
    "        pos_p = (self.pos_c_prob) #initialize the probability to prior\n",
    "        neg_p = (self.neg_c_prob)\n",
    "        \n",
    "        #loop through each word in positive \n",
    "        for f in feat_p:\n",
    "            if f[0] not in self.pos_prob: #if the word in data is not in our pos prob\n",
    "                prob_w = 1 #we need to account for prob_w being 1 \n",
    "                pos_p = pos_p * prob_w\n",
    "                \n",
    "            else:\n",
    "                prob_w = self.pos_prob[f[0]] #if word does exist in pos prob, pull probability of word \n",
    "                pos_p = pos_p * prob_w #calculate the probability \n",
    "        \n",
    "        #loop through each word in negative\n",
    "        for f in feat_n:\n",
    "            if f[0] not in self.neg_prob:#if the word in data is not in our neg prob\n",
    "                prob_w = 1 #we need to account for prob_w being 1 \n",
    "                neg_p = neg_p * prob_w\n",
    "            else:\n",
    "                prob_w = self.neg_prob[f[0]] #if word does exist in neg prob, pull probability of word \n",
    "                neg_p = neg_p * prob_w #calculate the probability \n",
    "        \n",
    "        self.score_class[0] = neg_p #update dictionary with positive and negative class probabilites\n",
    "        self.score_class[1] = pos_p\n",
    "        \n",
    "        return(self.score_class) #return dictionary as stated in writeup\n",
    "\n",
    "    def classify(self, data):\n",
    "        a = self.score(data)\n",
    "        if a[1] > a[0]: #if pos probability is greater\n",
    "            return 1 #we classify as positive\n",
    "        else: #if neg probability is greater\n",
    "            return 0 #classify as negative\n",
    "\n",
    "    def featurize(self, data): #pass one example (w1, 1)\n",
    "        sen = data[1] #we take the sentence: w1\n",
    "        label = data[2] #we then take the label: 1\n",
    "        tokens = sen.split() #we want to tokenize the sentence\n",
    "        feat = []\n",
    "        for b in tokens: #for each token in tokens, we create our feature with token and label\n",
    "            feat.append((b,label))\n",
    "        return feat #we will return a list of features\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Naive Bayes - bag-of-words baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "class SentimentAnalysisImproved:\n",
    "\n",
    "    def __init__(self):\n",
    "    # do whatever you need to do to set up your class here\n",
    "    \n",
    "        self.pos_count = {} #dictionary for pos_count -word count respect to positive comments (freq)\n",
    "        self.neg_count = {} #dictionary for neg_count -word count respect to negative comments (freq)\n",
    "        \n",
    "        self.vocab = 0 #number of words in our vocabulary\n",
    "        self.pos = 0 #class count for probability calculations \n",
    "        self.neg = 0 #class count for probability calculations\n",
    "        \n",
    "        self.pos_prob = {} #probability list for each positive word \n",
    "        self.neg_prob = {} #probability list for each negative word\n",
    "        \n",
    "        self.pos_c_prob = 0 #positive class probability\n",
    "        self.neg_c_prob = 0 #negative class probability\n",
    "        \n",
    "        self.score_class = {} #score return of dictionary P(c)*P(data|c)  for each class\n",
    "        \n",
    "\n",
    "        self.gold_labels = [0,0,0,1,0,0,1,1,1,1,1,0,1,0,0,0,1,0,1,0,0,1,1,0,1,1,0,1,1,1,0,0,0,0,1,0,1,1,1,1,0,0,1,0,1,1,1,1,1,0]\n",
    "        self.classified_labels = []\n",
    " \n",
    "        print(\"initialized improved\\n\")\n",
    "\n",
    "    def train(self, examples): #examples is the generated tuple we did above\n",
    "        for e in examples: #loop through all our examples\n",
    "            feat = self.featurize(e) #get our features using featurize= create a bag of word features (w1,1) -> (x,y)\n",
    "            #the example e can be positive or negative \n",
    "            if e[2] == 1: #if our label is 1--positive\n",
    "                self.pos += 1 #we want to add it to our positive count (class count) how many + classes we saw\n",
    "                for f in feat: #feat is a list of tuple, we need to look at each f in feat\n",
    "                    if f[0] not in self.pos_count: #if the token in that f is not present in dictionary\n",
    "                        #the first time we are encountering the word\n",
    "                        self.vocab += 1 #keep track of our vocab\n",
    "                        self.pos_count[f[0]] = 0 #put a zero for pos_count \n",
    "                        self.pos_count[f[0]] += 1 #increment count by one\n",
    "                    else: #if token is already present in dictionary we want to increment the count of token\n",
    "                        self.pos_count[f[0]] += 1\n",
    "                       \n",
    "            if e[2] == 0: #if our label is 0--negative\n",
    "                self.neg += 1 #we want to add it to our negative count (class count)\n",
    "                for f in feat:#repeat above\n",
    "                    if f[0] not in self.neg_count:\n",
    "                        self.neg_count[f[0]] = 0 #put a zero for neg_count \n",
    "                        self.neg_count[f[0]] += 1 #increment count by one\n",
    "                    else: \n",
    "                        self.neg_count[f[0]] += 1\n",
    "       \n",
    "        #we need to make sure that both dictionary has both words - dictionary needs same set of words\n",
    "        #If not in both, discard the word or handle the situation by adding to the other dictionary with a 0 count\n",
    "        \n",
    "        for k,v in self.neg_count.items(): #iterate over negative dictionary\n",
    "            if k not in self.pos_count: #if k is not present in positive dictionary\n",
    "                self.vocab += 1 #sum up the rest of vocab count with neg words as well\n",
    "                self.pos_count[k] = 0 #add k of neg vocab with a 0 count - important for laplace smoothing\n",
    "        for k,v in self.pos_count.items(): #do the same thing but we dont need to count vocab since we have all - and +\n",
    "            if k not in self.neg_count:\n",
    "                self.neg_count[k] = 0 #add k of pos vocab with a 0 count to neg_count \n",
    "        \n",
    "        #calculate probabilities\n",
    "        #the word count of w1 + 1/all the words in positive context + |v|\n",
    "        for k,v in self.pos_count.items():\n",
    "            self.pos_prob[k] = (self.pos_count[k] + 1)/(self.pos + self.vocab)\n",
    "            \n",
    "        for k,v in self.neg_count.items(): #same with words in negative context + |v|\n",
    "            self.neg_prob[k] = (self.neg_count[k] + 1)/(self.neg + self.vocab)\n",
    "        \n",
    "        #calculate class probabilities\n",
    "        self.pos_c_prob = self.pos/(self.pos + self.neg) #take all pos class counts/ total counts\n",
    "        self.neg_c_prob = self.neg/(self.pos + self.neg) #take all neg class counts/ total counts\n",
    "\n",
    "    def score(self, data):\n",
    "        \n",
    "        data_p = (0, data, 1) #create a tuple variable with id, sentence, and positive label\n",
    "        data_n = (0, data, 0) #create a tuple variable with id, sentence, and negative label\n",
    "        \n",
    "        feat_p = self.featurize(data_p) #featurize each positive and negative sentence data\n",
    "        feat_n = self.featurize(data_n)\n",
    "        \n",
    "        pos_p = (self.pos_c_prob) #initialize the probability to prior\n",
    "        neg_p = (self.neg_c_prob)\n",
    "        \n",
    "        #loop through each word in positive \n",
    "        for f in feat_p:\n",
    "            if f[0] not in self.pos_prob: #if the word in data is not in our pos prob\n",
    "                prob_w = 1 #we need to account for prob_w being 1 \n",
    "                pos_p = pos_p * prob_w\n",
    "                \n",
    "            else:\n",
    "                prob_w = self.pos_prob[f[0]] #if word does exist in pos prob, pull probability of word \n",
    "                pos_p = pos_p * prob_w #calculate the probability \n",
    "        \n",
    "        #loop through each word in negative\n",
    "        for f in feat_n:\n",
    "            if f[0] not in self.neg_prob:#if the word in data is not in our neg prob\n",
    "                prob_w = 1 #we need to account for prob_w being 1 \n",
    "                neg_p = neg_p * prob_w\n",
    "            else:\n",
    "                prob_w = self.neg_prob[f[0]] #if word does exist in neg prob, pull probability of word \n",
    "                neg_p = neg_p * prob_w #calculate the probability \n",
    "        \n",
    "        self.score_class[0] = neg_p #update dictionary with positive and negative class probabilites\n",
    "        self.score_class[1] = pos_p\n",
    "        \n",
    "        return(self.score_class) #return dictionary as stated in writeup\n",
    "\n",
    "    def classify(self, data):\n",
    "        a = self.score(data)\n",
    "        if a[1] > a[0]: #if pos probability is greater\n",
    "            return 1 #we classify as positive\n",
    "        else: #if neg probability is greater\n",
    "            return 0 #classify as negative\n",
    "\n",
    "    def featurize(self, data): #pass one example (w1, 1)\n",
    "        \n",
    "        stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \n",
    "                     \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \n",
    "                     \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\",\n",
    "                     \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \n",
    "                     \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
    "                     \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\",\n",
    "                     \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\",\n",
    "                     \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
    "                     \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "                     \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n",
    "                     \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\",\n",
    "                     \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "        sen = data[1] #we take the sentence: w1\n",
    "        label = data[2] #we then take the label: 1\n",
    "        tokens = sen.split() #we want to tokenize the sentence\n",
    "        feat = []\n",
    "        for b in tokens: #for each token in tokens, we create our feature with token and label\n",
    "            b.lower() #lowercase all words\n",
    "            b = punctuation(b) #remove punctuation in all the words\n",
    "            if b not in stopwords: #if words in stopwords, we won't add them to our feature\n",
    "                feat.append((b,label))\n",
    "        return feat #we will return a list of features\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ATTEMPT AT IMPROVEMENT!!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "\n",
      "initialized improved\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-d00f895d2f8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentAnalysisImproved\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgenerate_tuples_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0msa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-179-e0869d62499f>\u001b[0m in \u001b[0;36mgenerate_tuples_from_file\u001b[1;34m(training_file_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_tuples_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_file_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#use with statement to open the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#read the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#read each line in the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m#iterate over and create a list of tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "    if __name__ == \"__main__\":\n",
    "        if len(sys.argv) != 3:\n",
    "            print(\"Usage:\", \"python hw3_sentiment.py training-file.txt testing-file.txt\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        training = sys.argv[1]\n",
    "        testing = sys.argv[2]\n",
    "\n",
    "        sa = SentimentAnalysis()\n",
    "        im = SentimentAnalysisImproved()\n",
    "\n",
    "        f =  generate_tuples_from_file(training)\n",
    "\n",
    "        sa.train(f)\n",
    "        im.train(f)\n",
    "        print(\"Starting training of train data: \")\n",
    "        \n",
    "        print(\"Probability of each positive word in positive class (likelihood): \")\n",
    "        print(sa.pos_prob)\n",
    "        print(\"Probability of positive class (prior): \", sa.pos_c_prob, '\\n')\n",
    "\n",
    "        print(\"Probability of each negative word in negative class (likelihood): \")\n",
    "        print(sa.neg_prob)\n",
    "        print(\"Probability of negative class (prior): \", sa.neg_c_prob, '\\n')\n",
    "\n",
    "        print(\"Dictionary of the values of P(c)*P(data|c) for each class for answer check purposes: \")\n",
    "        print(\"Positive - I loved the hotel : \", sa.score(\"I loved the hotel\"))\n",
    "        print(\"Negative - I hated the hotel: \", sa.score(\"I hated the hotel\"), '\\n')\n",
    "\n",
    "        print(\"Classification print for answer check purposes: \")\n",
    "        print(\"Positive - I loved the hotel a lot: \", sa.classify(\"I loved the hotel a lot\"))\n",
    "        print(\"Negative - I hated the hotel: \", sa.classify(\"I hated the hotel\"), '\\n')\n",
    "\n",
    "        y = generate_test(testing)\n",
    "        z = open(\"label_test_data.txt\", \"w\")\n",
    "        z1 = open(\"label_test_data_improved.txt\", \"w\")\n",
    "        \n",
    "        for line in y:\n",
    "            label = sa.classify(line[1])\n",
    "            label1 = im.classify(line[1])\n",
    "            z.write(line[0] + \" \" + str(label) + '\\n')\n",
    "            z1.write(line[0] + \" \" + str(label1) + '\\n')\n",
    "            sa.classified_labels.append(label)\n",
    "            im.classified_labels.append(label1)\n",
    "        z.close()\n",
    "        z1.close()\n",
    "        print(\"Label output created and named: label_test_data.txt and label_test_data_improved.txt..... Generation of labels complete\")\n",
    "        print(\"Created Labels: \", sa.classified_labels)\n",
    "        print(\"Created Improved Labels: \", im.classified_labels, '\\n')\n",
    "        \n",
    "        print(\"Precision Unimproved: \", precision(sa.gold_labels, sa.classified_labels))\n",
    "        print(\"Recall Unimproved: \", recall(sa.gold_labels, sa.classified_labels))\n",
    "        print(\"F1 Unimproved: \", f1(sa.gold_labels, sa.classified_labels), '\\n')\n",
    "    \n",
    "        print(\"Precision Improved: \", precision(im.gold_labels, im.classified_labels))\n",
    "        print(\"Recall Improved: \", recall(im.gold_labels, im.classified_labels))\n",
    "        print(\"F1 Improved: \", f1(im.gold_labels, im.classified_labels), '\\n')\n",
    "      # do the things that you need to with your base class\n",
    "\n",
    "        #improved = SentimentAnalysisImproved()\n",
    "        #print(improved)\n",
    "      # do the things that you need to with your improved class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Testing if code works done below: \\n\")\n",
    "\n",
    "s = SentimentAnalysis()\n",
    "s1 = SentimentAnalysisImproved()\n",
    "f = generate_tuples_from_file(\"dev_file.txt\")\n",
    "\n",
    "s.train(f)\n",
    "s1.train(f)\n",
    "\n",
    "print(\"Probability of each positive word in positive class (likelihood): \")\n",
    "print(s.pos_prob, '\\n')\n",
    "print(\"Probability of positive class (prior): \")\n",
    "print(s.pos_c_prob, '\\n')\n",
    "\n",
    "print(\"Probability of each negative word in negative class (likelihood): \")\n",
    "print(s.neg_prob, '\\n')\n",
    "print(\"Probability of negative class (prior): \")\n",
    "print(s.neg_c_prob, '\\n')\n",
    "\n",
    "print(\"Testing dictionary of the values of P(c)*P(data|c) for each class: \")\n",
    "print(\"Positive: \", s.score(\"I loved the hotel\"))\n",
    "print(\"Negative: \", s.score(\"I hated the hotel\"), '\\n')\n",
    "\n",
    "print(\"Testing Classification: \")\n",
    "print(\"Positive - I loved the hotel a lot: \", s.classify(\"I loved the hotel a lot\"))\n",
    "print(\"Negative - I hated the hotel: \", s.classify(\"I hated the hotel\"), '\\n')\n",
    "\n",
    "y = generate_test(\"HW3-testset.txt\")\n",
    "z = open(\"label_test_data.txt\", \"w\")\n",
    "z1 = open(\"label_test_data_improved.txt\", \"w\")\n",
    "for line in y:\n",
    "    label = s.classify(line[1])\n",
    "    label1 = s1.classify(line[1])\n",
    "    z.write(line[0] + \" \" + str(label) + '\\n')\n",
    "    z1.write(line[0] + \" \" + str(label1) + '\\n')\n",
    "    s.classified_labels.append(label)\n",
    "    s1.classified_labels.append(label1)\n",
    "    \n",
    "z.close()\n",
    "\n",
    "print(\"Label output created and named: label_test_data.txt\")\n",
    "print(\"Gold Labels: \", s.gold_labels)\n",
    "print(\"Created Labels: \", s.classified_labels)\n",
    "print(\"Created Improved Labels: \", s1.classified_labels, '\\n')\n",
    "\n",
    "print(\"Precision Unimproved: \", precision(s.gold_labels, s.classified_labels))\n",
    "print(\"Recall Unimproved: \", recall(s.gold_labels, s.classified_labels))\n",
    "print(\"F1 Unimproved: \", f1(s.gold_labels, s.classified_labels), '\\n')\n",
    "        \n",
    "print(\"Precision Improved: \", precision(s1.gold_labels, s1.classified_labels))\n",
    "print(\"Recall Improved: \", recall(s1.gold_labels, s1.classified_labels))\n",
    "print(\"F1 Improved: \", f1(s1.gold_labels, s1.classified_labels), '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
